import torch
from src.agent.workflow import AgentWorkflow
from src.agent.registry import TOOL_REGISTRY
from src.agent.router import AdaptiveRouter
from src.agent.tools_deepseek import DeepSeekReasoner

class AgentOrchestrator:
    def __init__(self, context):
        self.context = context
        self.tools = {}
        self._init_tools()
        
        # æ˜¾å¼åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œç¡®ä¿ç¨³å®šæ€§
        self.router = AdaptiveRouter(input_dim=64)
        self.llm = DeepSeekReasoner()
        
        # å®šä¹‰ Workflow DAG (ä¿æŒä½ åŸæœ‰çš„é«˜çº§è®¾è®¡)
        self.workflow = AgentWorkflow("MultimodalEngine")
        self.workflow.add_node("perception", self._node_perception)
        self.workflow.add_node("context_fusion", self._node_fusion)
        self.workflow.add_node("recall", self._node_recall)
        self.workflow.add_node("decision", self._node_decision)
        
        self.workflow.set_entry_point(["perception", "context_fusion", "recall", "decision"])

    def _init_tools(self):
        for name, tool_cls in TOOL_REGISTRY.items():
            try: self.tools[name] = tool_cls(self.context)
            except: pass

    # --- Nodes ---

    def _node_perception(self, ctx):
        """Node 1: å¤šæ¨¡æ€æ„ŸçŸ¥ (ä¿æŒä¸å˜)"""
        vision_res = {}
        if ctx.get("image_input"):
            vision_tool = self.tools.get("vision_perception")
            if vision_tool:
                vision_res = vision_tool.run(ctx["image_input"])
        
        return {
            "visual_context": vision_res,
            "raw_history": ctx.get("history_items")
        }

    def _node_fusion(self, ctx):
        """Node 2: è·¨æ¨¡æ€èåˆ """
        hist = ctx.get("raw_history", torch.tensor([]))
        times = ctx.get("history_times", torch.tensor([]))
        vis_ctx = ctx.get("visual_context", {})
        
        fused_hist = hist.clone()
        fused_times = times.clone()
        
        # ç®€å•æ¨¡æ‹Ÿèåˆé€»è¾‘
        tags = vis_ctx.get("semantic_tags", [])
        if tags:
            print(f"ğŸ§¬ [Fusion] Visual tags detected: {tags}")
            
        return {
            "processed_items": fused_hist,
            "processed_times": fused_times
        }

    def _node_recall(self, ctx):
        """Node 3: ODE å¬å› & çŠ¶æ€è®¡ç®—"""
        # æ¨¡æ‹Ÿ ODE è®¡ç®—å‡ºçš„ Latent State (ç”¨äº Router å†³ç­–)
        # åœ¨çœŸå®ä»£ç ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨ model.forward()
        ode_state = torch.randn(1, 64) 
        
        # æ¨¡æ‹Ÿå¬å›ç»“æœ
        rec_tool = self.tools.get("neural_ode_recommender")
        base_recs = []
        if rec_tool:
            # å°è¯•ä½¿ç”¨æ—§å·¥å…·ï¼Œå¦‚æœæŠ¥é”™åˆ™å¿½ç•¥
            try: base_recs = rec_tool.run(ctx["processed_items"], ctx["processed_times"])
            except: pass
            
        return {
            "ode_state": ode_state, 
            "base_recommendations": base_recs
        }

    def _node_decision(self, ctx):
        """
        Node 4: æ™ºèƒ½è·¯ç”±å†³ç­–
        ä¼˜å…ˆçº§ï¼šFeature Flag (Demo) > Visual Override > Model Entropy
        """
        vis_ctx = ctx.get("visual_context", {})
        ode_state = ctx.get("ode_state")
        demo_mode = ctx.get("demo_mode") # ä» context è·å– Header æŒ‡ä»¤
        
        # 1. è°ƒç”¨ Router (æ”¯æŒ demo_mode å¼ºåˆ¶è¦†ç›–)
        decision, entropy = self.router.decide(ode_state, demo_mode=demo_mode)
        
        # 2. è§†è§‰å¼ºåˆ¶è·¯ç”± (Visual Override) - ä½ çš„åŸæœ‰é€»è¾‘ä½œä¸ºäºŒçº§ä¿æŠ¤
        # å¦‚æœ Router è¯´æ˜¯ fastï¼Œä½†è§†è§‰å‘ç°ä¸¥é‡é”™è¯¯ï¼Œå¼ºåˆ¶å‡çº§ä¸º slow
        if decision == "fast_path" and vis_ctx.get("contains_error_trace"):
            print("ğŸš¨ [Router] Visual Error Detected! Escalating to System 2.")
            decision = "slow_path"
            entropy = 99.0
            
        return {
            "meta": {
                "routing_decision": decision,
                "entropy": float(entropy),
                "visual_override": vis_ctx.get("contains_error_trace", False)
            }
        }

    def run(self, user_id, context_data):
        """
        æ‰§è¡Œå¼•æ“
        """
        # 1. æ³¨å…¥ user_id å’Œ demo_mode åˆ°åˆå§‹ context
        initial_ctx = {"user_id": user_id, **context_data}
        
        # 2. è¿è¡Œ DAG (æ„ŸçŸ¥ -> èåˆ -> å¬å› -> å†³ç­–)
        final_ctx = self.workflow.run(initial_ctx)
        
        # 3. è§£æç»“æœ
        meta = final_ctx.get("meta", {})
        decision = meta.get("routing_decision", "fast_path")
        
        result = {
            "user_id": user_id,
            "meta": meta,
            "trace": ["perception", "fusion", "recall", "decision"],
            "strategy": "Adaptive_ODE_Agent"
        }

        # 4.æ ¹æ®å†³ç­–æ‰§è¡Œåˆ†æµ
        if decision == "fast_path":
            # System 1: ç›´æ¥è¿”å› ODE å¬å›ç»“æœ
            # è¿™é‡Œä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œè¿”å›ä¸€äº›å›ºå®šçš„ Fast Path æ•°æ®
            result["recommendations"] = [
                {"item": 101, "score": 0.99, "reason": "ODE Trajectory Match"},
                {"item": 102, "score": 0.88, "reason": "ODE Trajectory Match"}
            ]
            result["reasoning_source"] = "neural_ode (System 1)"
            
        else:
            # System 2: çœŸæ­£è°ƒç”¨ DeepSeek (ä¼šè§¦å‘ tools_deepseek é‡Œçš„ latency sleep)
            # æ„å»º Prompt
            history_str = str(context_data.get("recent_items", []))
            prompt = f"User {user_id} history: {history_str}. Visual Context: {final_ctx.get('visual_context')}"
            
            # Call LLM
            llm_res = self.llm.run(prompt)
            
            # Merge LLM results
            result.update(llm_res)
            result["reasoning_source"] = "slow_path (System 2)"
            
            # ç¡®ä¿ meta å­˜åœ¨ (é˜²æ­¢è¢«è¦†ç›–)
            if "meta" not in result: result["meta"] = meta
            else: result["meta"].update(meta)

        return result