import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import sys
import os
import time

# Path Hack: ç¡®ä¿èƒ½æ‰¾åˆ° src
sys.path.append(os.getcwd())

# å¼•å…¥æˆ‘ä»¬æ•´ç†å¥½çš„æ¨¡å—
# [Fix] è¿™é‡Œçš„ç±»åè¦å’Œ src/data/dataset.py é‡Œçš„ä¸€è‡´
from src.data.dataset import UserBehaviorDataset 
from src.dynamics.modeling import DeepM3Model
from src.utils.seeder import set_seed
from src.utils.env_check import print_env_fingerprint

def train(args):
    # 1. ç¯å¢ƒå‡†å¤‡
    print_env_fingerprint() # [Task 8] æ‰“å°ç¯å¢ƒæŒ‡çº¹
    set_seed(args.seed)     # [Task 8] é”å®šç§å­
    
    device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ğŸš€ Training on Device: {device}")

    # 2. åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    # 3. æ•°æ®å‡†å¤‡
    print("ğŸ”„ Loading Data...")
    try:
        train_dataset = UserBehaviorDataset(mode='train', config=config)
        test_dataset = UserBehaviorDataset(mode='test', config=config)
    except Exception as e:
        print(f"âŒ Data Load Error: {e}")
        print("ğŸ’¡ Tip: Did you run 'python src/data/preprocessor.py' first?")
        return

    # [Fix] è·å– n_items ç”¨äºæ¨¡å‹åˆå§‹åŒ–
    n_items = train_dataset.n_items
    print(f"ğŸ“Š Items: {n_items} | Train Size: {len(train_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=config['train']['batch_size'], shuffle=False)

    # 4. æ¨¡å‹åˆå§‹åŒ–
    model = DeepM3Model(config, n_items=n_items).to(device)
    lr = float(config['train'].get('learning_rate', 1e-3))
    weight_decay = float(config['train'].get('weight_decay', 1e-5))
    
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()

    # 5. è®­ç»ƒå¾ªç¯
    print(f"ğŸ”¥ Start Training for {args.epochs} Epochs...")
    best_loss = float('inf')
    
    for epoch in range(args.epochs):
        model.train()
        total_loss = 0
        start_time = time.time()
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            x = batch['x'].to(device) # [B, Seq]
            t = batch['t'].to(device) # [B, Seq]
            y = batch['y'].to(device) # [B] (Next Item Label)
            
            # Forward
            logits = model(x, t) # [B, n_items]
            
            # è¿™ç§ç®€å•çš„ Auto-regressive ä»»åŠ¡é€šå¸¸å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ª
            # è¿™é‡Œçš„ logits å·²ç»æ˜¯ head è¾“å‡ºçš„ [B, n_items]
            
            loss = criterion(logits, y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª (ODE è®­ç»ƒç¨³å®šæ€§å…³é”®)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            total_loss += loss.item()
            
        avg_loss = total_loss / len(train_loader)
        epoch_time = time.time() - start_time
        
        print(f"   Epoch {epoch+1}/{args.epochs} | Loss: {avg_loss:.4f} | Time: {epoch_time:.1f}s")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            # [Fix] ä¿å­˜ä¸ºæˆ‘ä»¬åœ¨ README é‡Œæ‰¿è¯ºçš„æ–‡ä»¶å
            save_path = "checkpoints/model_ode_rk4.pth"
            os.makedirs("checkpoints", exist_ok=True)
            torch.save(model.state_dict(), save_path)
            
    print(f"âœ… Training Complete. Best Loss: {best_loss:.4f}")
    print(f"ğŸ’¾ Model saved to: {save_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='configs/config.yaml')
    parser.add_argument('--epochs', type=int, default=5) # é»˜è®¤æ”¹ä¸º5
    parser.add_argument('--seed', type=int, default=42)
    args = parser.parse_args()
    
    train(args)