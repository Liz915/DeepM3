import os
import torch
import json
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig

# ==========================================
# ğŸ¯ é…ç½®åŒºåŸŸ (Mac M1/M2/M3 æœ€ç»ˆç¨³å®šç‰ˆ)
# ==========================================
MODEL_ID = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "checkpoints/dpo_qwen25_final"
DATA_FILE = "assets/dpo_dataset_final.jsonl"

# è®­ç»ƒé…ç½®
NUM_EPOCHS = 3
BATCH_SIZE = 1           # M1 æ˜¾å­˜åƒç´§ï¼ŒBatch Size åªèƒ½è®¾ 1
GRAD_ACCUM = 8           # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆ Batch Size = 8
LEARNING_RATE = 1e-5     # DPO æ ‡å‡†å­¦ä¹ ç‡

def main():
    # 1. è®¾å¤‡æ£€æµ‹
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"ğŸš€ Running DPO on {device.upper()} ...")

    # 2. åŠ è½½æ•°æ®
    if not os.path.exists(DATA_FILE):
         raise FileNotFoundError(f"Data file {DATA_FILE} not found!")
    
    data_list = []
    with open(DATA_FILE, "r") as f:
        for line in f:
            try:
                row = json.loads(line)
                if "prompt" in row and "chosen" in row and "rejected" in row:
                    data_list.append(row)
            except: pass

    dataset = Dataset.from_list(data_list)
    print(f"ğŸ“š Loaded {len(dataset)} valid samples.")

    # 3. Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # 4. åŠ è½½æ¨¡å‹
    # ä½¿ç”¨ torch.float32 ä¿è¯ MPS ç»å¯¹ç¨³å®š (0.5B æ¨¡å‹ FP32 ä¹Ÿå°± 2GB æ˜¾å­˜ï¼ŒM1 æ‰›å¾—ä½)
    print("ğŸ¤– Loading Model (FP32)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float32, 
        device_map=None 
    ).to(device)

    # 5. LoRA é…ç½® (å…¨é‡ Linear å±‚)
    # è¡¥å…¨ Qwen çš„ MLP å±‚ï¼Œæ•ˆæœæ›´å¥½
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj", 
            "gate_proj", "up_proj", "down_proj"
        ],
        use_dora=False # å…³é—­ DoRAï¼Œæå‡ MPS è®­ç»ƒé€Ÿåº¦
    )

    # 6. è®­ç»ƒå‚æ•°
    training_args = DPOConfig(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        
        # å…³é—­æ··åˆç²¾åº¦ï¼Œé˜²æ­¢ MPS æŠ¥é”™ (é€Ÿåº¦æ…¢ç‚¹ä½†èƒ½è·‘å®Œ)
        fp16=False,
        bf16=False,
        
        logging_steps=1,
        save_steps=50,
        report_to="tensorboard",
        remove_unused_columns=False,
        max_prompt_length=512,
        max_length=1024,
    )

    # 7. åˆå§‹åŒ– Trainer
    print("ğŸ”¥ Initializing DPO Trainer...")
    trainer = DPOTrainer(
        model=model,
        ref_model=None, # æ˜¾å¼è®¾ä¸º Noneï¼Œè®© TRL å†…éƒ¨å¤„ç† Reference
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer, # å‚æ•°åå·²ä¿®æ­£
        peft_config=peft_config,
    )

    # 8. å¼€å§‹è®­ç»ƒ
    print("ğŸï¸ Start Training! (This may take a while on M1...)")
    trainer.train()

    # 9. ä¿å­˜
    print("ğŸ’¾ Saving LoRA adapter...")
    trainer.save_model(OUTPUT_DIR)

if __name__ == "__main__":
    main()